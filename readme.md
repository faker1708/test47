


计划

自收敛的价值函数 


整理mlp类，dqn类。

23 04 07


训练 多了，就感觉，它会更贴合我们的指标。但本来它可以做得更好的，结果，现在变成仅仅及格了。。。


如果在对局 结束后，一口气训练16次，则表现并不好。经常会几十个step就挂掉。但如果不挂，则还挺稳定。也就是表现得不可靠。


多少有点过拟合的感觉，感觉它对某些场景特别上手。但某些场景完全不会玩的样子。
如果在对局 结束后，一口气训练4次，最后测试成绩还不错。






我还尝试了，当它通过阶段0后，（也就是能跑出几千分的成绩后），后面的阶段只让它跑几百次。
这样训练 快很多，但成绩也挺灾难的。

所以，训练 这块，还是什么都别改了。


现在唯一有用的修改就是简单加了tlun，也就是分阶段训练。

现在不改进算法，而强行提高对x的要求，（它的方差依然比较大）它就会挂掉。
基本都是角度挂掉的。也很好理解 。我们对角度提要求。。

tlun 多了，比如0.24档重复也次，这样也不好。测试时容易挂。挂角度。


原因就是它收敛太多了，x会变得很小 ，但这是角度就会扭得太厉害，也就是算法太关注x，没有关注角度，x要求太高，角度受不了了。

所以，毕业考试时，可能 要筛掉那些 x太小的。也就是成绩特别好的学生，因为那样角度太振荡。


现在的问题，当我们一步一步，加强要求时，原来可以去的地方，现在应当标注为禁地。比如原来 2.3也可以去，现在就不应该再让学生去了，不然它能去0.7，却跑去2.3 纯浪费时间。反而那些撞到游戏失败的学生，能迅速找到收敛的方向。


所以确实有必要收窄生存的范围。

