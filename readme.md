


计划

自收敛的价值函数 


整理mlp类，dqn类。

23 04 07


训练 多了，就感觉，它会更贴合我们的指标。但本来它可以做得更好的，结果，现在变成仅仅及格了。。。


如果在对局 结束后，一口气训练16次，则表现并不好。经常会几十个step就挂掉。但如果不挂，则还挺稳定。也就是表现得不可靠。


多少有点过拟合的感觉，感觉它对某些场景特别上手。但某些场景完全不会玩的样子。
如果在对局 结束后，一口气训练4次，最后测试成绩还不错。






我还尝试了，当它通过阶段0后，（也就是能跑出几千分的成绩后），后面的阶段只让它跑几百次。
这样训练 快很多，但成绩也挺灾难的。

所以，训练 这块，还是什么都别改了。


现在唯一有用的修改就是简单加了tlun，也就是分阶段训练。

现在不改进算法，而强行提高对x的要求，（它的方差依然比较大）它就会挂掉。
基本都是角度挂掉的。也很好理解 。我们对角度提要求。。

tlun 多了，比如0.24档重复也次，这样也不好。测试时容易挂。挂角度。


原因就是它收敛太多了，x会变得很小 ，但这是角度就会扭得太厉害，也就是算法太关注x，没有关注角度，x要求太高，角度受不了了。

所以，毕业考试时，可能 要筛掉那些 x太小的。也就是成绩特别好的学生，因为那样角度太振荡。


现在的问题，当我们一步一步，加强要求时，原来可以去的地方，现在应当标注为禁地。比如原来 2.3也可以去，现在就不应该再让学生去了，不然它能去0.7，却跑去2.3 纯浪费时间。反而那些撞到游戏失败的学生，能迅速找到收敛的方向。


所以确实有必要收窄生存的范围。

今天的目标，尝试给出
训练 一个不遵循智人写的价值函数的机器算法。




1   匹敌的对手
2   强力的指导

这样能产生有奖励的经验。

用这样带奖励的数据，去训练一个新机器。

现在有个问题，拿到奖励的前期的操作，能否能直接被 机器识别，还是说我需要手动添加奖励去？


我知道了，需要我手动添加。

还是再学一下蒙卡搜索树吧。空想太难了，要学知识再思考 。

它存在一个问题，如果仿真到某个状态是输掉了，就要向上报告给所有的前状态。也就是上级结点。
所有的结点 的价值都要加上这次的奖励。

我有点明白蒙树的意思 了，就是仿真，用树来记录仿真结果 。就没啥了。


神经网络 现在变成有损压缩 了。。。

妈的，手机是真容易 分心，想做事，就不要拿起你的手机。


它好像在说，仿真一次，并不是把节点加进来了；扩展操作才是加节点。

没有探索过，就仿真；探索过了，就扩展。

对。仿真是根据神经网络的指导来走，还是瞎比乱走呢。要么还是像之前那样epsilon贪心吧。


仿真的时候不就能加节点了吗?



不行，有点吃力，先写点东西吧。别学了。



现在，要把各个类整理整洁。
把接口写好，每个类需要什么参数。


mlp，
输入，神经网络结构 。

方法
建立神经网络 
前向
计算损失
更新



dqn
参数：神经网络结构

方法：
存储
学习
决策


封装代码的经验
1   全局变量是封装的灾难
2   先把各个类写在一个文件内，调试ok了再拆开到不同的文件

手写mlp又失败了。不行。
算了吧。用别人封装的吧。



